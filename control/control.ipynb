{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFQ_Example_DNN_LSTM_Qcontrol_application.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_gMI4Blbe8",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Quantum Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bogCr-sSkXM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wppQ3TJ23mWC",
        "colab_type": "text"
      },
      "source": [
        "# A Hybrid Quantum-Classical Optimization for Quantum Control Optimization\n",
        "\n",
        "\n",
        "Now that the basics are understood, let's show how one might use TFQ to construct a **hybrid quantum-classical neural net** for quantum control. \n",
        "\n",
        "\n",
        "In the frist problem, we train a classical deep neural net to control a single qubit for realizing an arbitrary unitary; the output of the classical neural network determines the parameters of the quantum circuit to be applied to the qubit, which is then measured to produce the expectation values of different measurement operators.\n",
        "\n",
        "In the second problem, we train a recurrent neural network, to learn to predict the future quantum dynamics based on past obervations of a noisy implementation of quantum circuit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngDCx3sUlmlA",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/quantum/blob/research/control/control.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJa9sPPckfqS",
        "colab_type": "text"
      },
      "source": [
        "## Installation and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eVDbG_2ZhMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade cirq==0.7.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFqxhKypZoSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcDb1zbSdXKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tfq-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZ300Bflq80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "import cirq\n",
        "import sympy\n",
        "import cmath\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "\n",
        "# visualization tools\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J6c2-DVXQdr",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1: Gate decomposition with DNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfSYDfAcJTB",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Problem definition.\n",
        "\n",
        "More specifically, we provide an example of training a classical neural network to learn the Bloch Theorem: any single qubit unitary transformation can be realized by three single qubit rotations around two different angles.\n",
        "\n",
        "\n",
        "Given the specification of the single qubit unitary in regard to three parameters $\\phi, \\theta_1, \\theta_2$ that specifies the block sphere as the input to the classical neural network: the neural network will output rotation angles along two different axis ( $Ry$, and $Rz$) that realizes the given rotation. If we include a penalty term on the number of non-zero rotations, the training of a classical neural network should be able to find the most optimal.\n",
        "\n",
        "<img src=\"https://i.imgur.com/LX134rt.png\" width=\"1000\">\n",
        "\n",
        "This idealized version of a real quantum control problem where a classical neural network is learning a physical law of optimal decomposition of single-qubit unitaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2TQJMt1HOhF",
        "colab_type": "text"
      },
      "source": [
        "Up to a global phase, an arbitrary single-qubit unitary can be specified in terms of three angles $\\phi$, $\\theta_1$, and $\\theta_2$ as $U=\\exp(-i \\phi (\\cos(\\theta_1)Z + \\sin(\\theta_1)(\\cos(\\theta_2)X + \\sin(\\theta_2)Y))$.\n",
        "\n",
        "It is possible to realize an arbitrary unitary of this form using three rotations about only two axes, $U = R_z(\\beta)R_y(\\gamma)R_z(\\delta)$.  There exists an analytic solution mapping $\\{\\phi, \\theta_1, \\theta_2\\}$ to $\\{\\beta, \\gamma, \\delta\\}$; however, for more sophisticated control problems, such an analytic mapping may not be available.  Therefore we investigate training neural networks to perform the control.  First we consider training a purely classical neural network to perform the mapping from unitary parameters to rotation angles.  Then, we consider training a hybrid quantum-classical network directly on expectation value data.\n",
        "\n",
        "First, we define the map from $\\{\\phi, \\theta_1, \\theta_2\\}$ to the associated unitary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU_dl3iPVylT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_unitary_from_params(phi, theta_1, theta_2):\n",
        "  return linalg.expm(-1j*phi*(\n",
        "      np.cos(theta_1)*cirq.Z._unitary_()\n",
        "      + np.sin(theta_1)*(\n",
        "          np.cos(theta_2)*cirq.X._unitary_()\n",
        "          + np.sin(theta_2)*cirq.Y._unitary_()\n",
        "      )\n",
        "  ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjQXFVuM4Kim",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Physics model construction\n",
        "\n",
        "#### Following lines are analytic solution to the Bloch theorem, where batch input is the input to a unitary parameterized by $U=\\exp(-i \\phi (cos(\\theta_1)Z + \\sin(\\theta_1)(\\cos(\\theta_2)X + \\sin(\\theta_2)Y))$, the output is the angle $\\beta, \\gamma, \\delta$ for the Z Y Z rotations that realizes the target unitary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sJ7NKALZ8oJ",
        "colab_type": "text"
      },
      "source": [
        "Next, we write down the known mapping between the angles $\\{\\phi, \\theta_1, \\theta_2\\}$ and the two-axis control angles $\\{\\beta, \\gamma, \\delta\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPQkoK93HTFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_unitary_params(phi, theta_1, theta_2):\n",
        "  \"\"\"Convert unitary angles into two-axis control angles.\"\"\"\n",
        "  #### Below works ONLY when all input angles are less than pi\n",
        "\n",
        "  gamma = -2*np.arccos(-np.sqrt(3 + np.cos(2 * theta_1)  + 2 * np.sin(theta_1) ** 2 * np.cos(phi* 2) )/2.0)\n",
        "\n",
        "  delta = 2*np.real(-  1j * np.log(-(-1.0) ** (1/4) *np.sqrt(- np.exp(1j * phi) * (-1 + np.exp(2 * 1j * phi))   * \n",
        "                                                              np.sqrt(3 + np.cos(2 * theta_1)  + 2 * np.sin(theta_1) ** 2 * np.cos(phi* 2) )) /\n",
        "                                      2 / np.sqrt(-np.exp(1j * ( 2 *phi+ theta_2)) * np.sin(phi) ** 2 \n",
        "                                                  * (np.cos(theta_1) + 1j / np.tan(phi))) ))\n",
        "\n",
        "  beta = 2*np.real(-  1j * np.log(- np.exp(1j * ( phi+ theta_2)) * \n",
        "                                      np.sqrt(-1j * np.exp(1j * phi)*(-1 + np.exp(2 * 1j * phi))\n",
        "                                      *np.sqrt(3 + np.cos(2 * theta_1)  + 2 * np.sin(theta_1) ** 2 * np.cos(phi* 2) ))\n",
        "                                      * np.sin(phi)/(-1 + np.exp(2 * 1j * phi)) /\n",
        "                                      np.sqrt(- np.exp(1j * (2 * phi+ theta_2)) * np.sin(phi) *( np.sin(phi) * np.cos(theta_1) + 1j * np.cos(phi) )) ))\n",
        "\n",
        "  return beta, gamma, delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh0vypmLpLWe",
        "colab_type": "text"
      },
      "source": [
        "Build a function to generate training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPxwDSubpOcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles_training_data(batch_size):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for _ in range(batch_size):\n",
        "      random_unitary_params = np.random.uniform(0,  np.pi, (3)).tolist()\n",
        "      beta, gamma, delta = map_unitary_params(*random_unitary_params)\n",
        "      data.append(random_unitary_params)\n",
        "      labels.append([beta, gamma, delta])\n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KnTDJ138e5m",
        "colab_type": "text"
      },
      "source": [
        "Using this function, set up the training and validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unUaBFTo8hRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "\n",
        "train_size = 10000\n",
        "validation_size = 10000\n",
        "all_commands, all_expectations = get_expectation_training_data(train_size + validation_size)\n",
        "\n",
        "commands_train = all_commands[:train_size]\n",
        "expectations_train = all_expectations[:train_size]\n",
        "commands_val = all_commands[-validation_size:]\n",
        "expectations_val = all_expectations[-validation_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkcRVIXTjLx1",
        "colab_type": "text"
      },
      "source": [
        "Run a test to confirm that all training data is correct.  We do this by checking the Hilbert-Schmidt inner product $\\langle U_i, U_o\\rangle = \\text{Tr}\\left(U_o^\\dagger U_i\\right)$ between the two resulting matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYJ8MHPLjLU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data_angles, label_angles in zip(commands_train, all_expectations):\n",
        "  test_phi, test_theta_1, test_theta_2 = data_angles\n",
        "  beta = label_angles[0]\n",
        "  gamma = label_angles[1]\n",
        "  delta = label_angles[2]\n",
        "  u_i = get_unitary_from_params(test_phi, test_theta_1, test_theta_2)\n",
        "  u_o = np.matmul(cirq.Rz(beta)._unitary_(),\n",
        "      np.matmul(cirq.Ry(gamma)._unitary_(), cirq.Rz(delta)._unitary_()))\n",
        "  \n",
        "  circuit = cirq.Circuit(cirq.Rz(delta)(q), cirq.Ry(   gamma)(q), cirq.Rz(beta)(q))\n",
        " \n",
        "  check1= np.trace(np.matmul(u_o.conj().T, circuit.unitary())) ** 2 / 4.0\n",
        "\n",
        "  check = np.trace(np.matmul(u_o.conj().T, u_i))** 2 / 4.0\n",
        " \n",
        "\n",
        "  if (abs(abs(check) - 1) > 1e-5) and (abs(abs(check1) - 1) > 1e-5):\n",
        "    print(\"Inner product value:\")\n",
        "    print(check)\n",
        "    print(\"Input angles quadrant check:\")\n",
        "    print([int(test_phi>np.pi), int(test_theta_1>np.pi), int(test_theta_2>np.pi)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV85ahKKCuQw",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Prepare the training data set based on input and expectation values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFIbFhOgalLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_expectation_training_data(batch_size):\n",
        "    q = cirq.GridQubit(0, 0)\n",
        "    beta_s, gamma_s, delta_s = sympy.symbols(\"beta gamma delta\")\n",
        "    circuit = cirq.Circuit(cirq.Rz(delta_s)(q), cirq.Ry(gamma_s)(q), cirq.Rz(beta_s)(q))\n",
        "    ops = [cirq.X(q), cirq.Y(q), cirq.Z(q)]\n",
        "\n",
        "    params = []\n",
        "    outputs = []\n",
        "    for _ in range(batch_size):\n",
        "      random_unitary_params = np.random.uniform(0,  np.pi, (3)).tolist()\n",
        "      beta, gamma, delta = map_unitary_params(*random_unitary_params)\n",
        "      expectations = tfq.layers.Expectation()(\n",
        "          circuit,\n",
        "          symbol_names=[beta_s, gamma_s, delta_s],\n",
        "          symbol_values=[[beta, gamma, delta]],\n",
        "          operators=ops \n",
        "      ).numpy().tolist()[0]\n",
        "      params.append(random_unitary_params)\n",
        "      outputs.append(expectations)\n",
        "    return params, outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OLc2MB-jOuD",
        "colab_type": "text"
      },
      "source": [
        "We now define the hybrid network that will be trained to perform the qubit control.  Note that we restrict the gate set of the quantum portion of the net to alternating $R_z$ and $R_y$ gates.  By adding a term to the loss that induces sparsity on the controls of these gates, the hope is that the hybrid network will learn the optimal two-axis control (which requires only three angles)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUG8Hct01KUg",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Build a quantum-classical hybrid neural network to control the rotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEwrVek41MZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y_true,y_pred ):\n",
        "    return -tf.math.log(tf.reduce_mean(tf.square(y_pred - y_true), axis=-1)) #+ 0.1 * tf.reduce_sum(tf.square(tf.math.tanh(dense_3)), axis=-1)\n",
        "\n",
        "circuits_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string, name='circuits_input')\n",
        "commands_input = tf.keras.Input((3,), name='commands_input')\n",
        "dense_layer_1 = tf.keras.layers.Dense(128, activation='relu', name='dense_layer_1')(commands_input)\n",
        "dense_layer_2 = tf.keras.layers.Dense(128, name='dense_layer_2')(dense_layer_1)\n",
        "dense_layer_3 = tf.keras.layers.Dense(64, activation='relu', name='dense_layer_3')(dense_layer_2) \n",
        "angles_layer = tf.keras.layers.Dense(3,  activation='linear', name='angles_layer')(dense_layer_3)\n",
        "\n",
        "measured_expectations = tfq.layers.ControlledPQC(\n",
        "    two_axis_control_circuit, measure_list)([circuits_input, angles_layer])\n",
        "two_axis_control_model = tf.keras.Model(inputs=[circuits_input, commands_input], outputs=measured_expectations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0wVKys4kYw",
        "colab_type": "text"
      },
      "source": [
        "Set up data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRqW1lmkYEMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = 10000\n",
        "validation_size = 10000\n",
        "all_commands, all_expectations = get_expectation_training_data(train_size + validation_size)\n",
        "\n",
        "commands_train = all_commands[:train_size]\n",
        "expectations_train = all_expectations[:train_size]\n",
        "commands_val = all_commands[-validation_size:]\n",
        "expectations_val = all_expectations[-validation_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziAd64oNYH3D",
        "colab_type": "text"
      },
      "source": [
        "Perform optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig-95OfT2AYG",
        "colab_type": "code",
        "outputId": "4f717bdc-73a3-41c3-9846-139d915f2ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 200\n",
        "batch_size = 1000\n",
        "lr=0.010 \n",
        "\n",
        "two_axis_control_model.compile(tf.keras.optimizers.Adam(learning_rate=lr, decay=lr / epochs), \n",
        "              loss='mse')\n",
        "history_two_axis = two_axis_control_model.fit(\n",
        "    x=[tfq.convert_to_tensor([cirq.Circuit()]*train_size), tf.convert_to_tensor(commands_train)],\n",
        "    y=tf.convert_to_tensor(expectations_train), batch_size=batch_size, epochs=epochs,\n",
        "    validation_data=(\n",
        "        [tfq.convert_to_tensor([cirq.Circuit()]*validation_size), tf.convert_to_tensor(commands_val)],\n",
        "                               tf.convert_to_tensor(expectations_val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "10000/10000 [==============================] - 3s 347us/sample - loss: 0.3856 - val_loss: 0.3154\n",
            "Epoch 2/200\n",
            "10000/10000 [==============================] - 3s 256us/sample - loss: 0.2913 - val_loss: 0.2889\n",
            "Epoch 3/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.2715 - val_loss: 0.2741\n",
            "Epoch 4/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.2600 - val_loss: 0.2636\n",
            "Epoch 5/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.2463 - val_loss: 0.2428\n",
            "Epoch 6/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.2178 - val_loss: 0.2018\n",
            "Epoch 7/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.1782 - val_loss: 0.1624\n",
            "Epoch 8/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.1456 - val_loss: 0.1490\n",
            "Epoch 9/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.1142 - val_loss: 0.0980\n",
            "Epoch 10/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0842 - val_loss: 0.0779\n",
            "Epoch 11/200\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0790 - val_loss: 0.0871\n",
            "Epoch 12/200\n",
            "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0759 - val_loss: 0.0723\n",
            "Epoch 13/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0640 - val_loss: 0.0567\n",
            "Epoch 14/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0513 - val_loss: 0.0473\n",
            "Epoch 15/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0431 - val_loss: 0.0437\n",
            "Epoch 16/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0395 - val_loss: 0.0405\n",
            "Epoch 17/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0422 - val_loss: 0.0439\n",
            "Epoch 18/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0414 - val_loss: 0.0383\n",
            "Epoch 19/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0382 - val_loss: 0.0395\n",
            "Epoch 20/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0377 - val_loss: 0.0372\n",
            "Epoch 21/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0373 - val_loss: 0.0561\n",
            "Epoch 22/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0429 - val_loss: 0.0376\n",
            "Epoch 23/200\n",
            "10000/10000 [==============================] - 3s 277us/sample - loss: 0.0328 - val_loss: 0.0335\n",
            "Epoch 24/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0314 - val_loss: 0.0362\n",
            "Epoch 25/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0300 - val_loss: 0.0317\n",
            "Epoch 26/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0278 - val_loss: 0.0287\n",
            "Epoch 27/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0270 - val_loss: 0.0264\n",
            "Epoch 28/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0272 - val_loss: 0.0296\n",
            "Epoch 29/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0347 - val_loss: 0.0337\n",
            "Epoch 30/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0318 - val_loss: 0.0343\n",
            "Epoch 31/200\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0313 - val_loss: 0.0316\n",
            "Epoch 32/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0502 - val_loss: 0.0332\n",
            "Epoch 33/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0376 - val_loss: 0.0328\n",
            "Epoch 34/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0299 - val_loss: 0.0225\n",
            "Epoch 35/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0240 - val_loss: 0.0249\n",
            "Epoch 36/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0212 - val_loss: 0.0205\n",
            "Epoch 37/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0198 - val_loss: 0.0208\n",
            "Epoch 38/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0184 - val_loss: 0.0202\n",
            "Epoch 39/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0176 - val_loss: 0.0185\n",
            "Epoch 40/200\n",
            "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0204 - val_loss: 0.0200\n",
            "Epoch 41/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0216 - val_loss: 0.0228\n",
            "Epoch 42/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0198 - val_loss: 0.0201\n",
            "Epoch 43/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0181 - val_loss: 0.0193\n",
            "Epoch 44/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0199 - val_loss: 0.0724\n",
            "Epoch 45/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0506 - val_loss: 0.0326\n",
            "Epoch 46/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0280 - val_loss: 0.0246\n",
            "Epoch 47/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0208 - val_loss: 0.0217\n",
            "Epoch 48/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0182 - val_loss: 0.0188\n",
            "Epoch 49/200\n",
            "10000/10000 [==============================] - 3s 285us/sample - loss: 0.0168 - val_loss: 0.0173\n",
            "Epoch 50/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0167 - val_loss: 0.0176\n",
            "Epoch 51/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0170 - val_loss: 0.0169\n",
            "Epoch 52/200\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0185 - val_loss: 0.0185\n",
            "Epoch 53/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0177 - val_loss: 0.0172\n",
            "Epoch 54/200\n",
            "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0170 - val_loss: 0.0177\n",
            "Epoch 55/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0165 - val_loss: 0.0148\n",
            "Epoch 56/200\n",
            "10000/10000 [==============================] - 3s 280us/sample - loss: 0.0142 - val_loss: 0.0155\n",
            "Epoch 57/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0154 - val_loss: 0.0159\n",
            "Epoch 58/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0144 - val_loss: 0.0149\n",
            "Epoch 59/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0141 - val_loss: 0.0153\n",
            "Epoch 60/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0143 - val_loss: 0.0153\n",
            "Epoch 61/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0156 - val_loss: 0.0205\n",
            "Epoch 62/200\n",
            "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0155 - val_loss: 0.0191\n",
            "Epoch 63/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0175 - val_loss: 0.0148\n",
            "Epoch 64/200\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0156 - val_loss: 0.0145\n",
            "Epoch 65/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0132 - val_loss: 0.0136\n",
            "Epoch 66/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0131 - val_loss: 0.0142\n",
            "Epoch 67/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0128 - val_loss: 0.0132\n",
            "Epoch 68/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0128 - val_loss: 0.0152\n",
            "Epoch 69/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0128 - val_loss: 0.0131\n",
            "Epoch 70/200\n",
            "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0125 - val_loss: 0.0120\n",
            "Epoch 71/200\n",
            "10000/10000 [==============================] - 3s 276us/sample - loss: 0.0119 - val_loss: 0.0134\n",
            "Epoch 72/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0129 - val_loss: 0.0141\n",
            "Epoch 73/200\n",
            "10000/10000 [==============================] - 3s 279us/sample - loss: 0.0144 - val_loss: 0.0148\n",
            "Epoch 74/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0143 - val_loss: 0.0142\n",
            "Epoch 75/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0138 - val_loss: 0.0137\n",
            "Epoch 76/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0146 - val_loss: 0.0142\n",
            "Epoch 77/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0169 - val_loss: 0.0150\n",
            "Epoch 78/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0153 - val_loss: 0.0149\n",
            "Epoch 79/200\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0169 - val_loss: 0.0267\n",
            "Epoch 80/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0207 - val_loss: 0.0185\n",
            "Epoch 81/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0266 - val_loss: 0.0372\n",
            "Epoch 82/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0400 - val_loss: 0.0258\n",
            "Epoch 83/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0233 - val_loss: 0.0204\n",
            "Epoch 84/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0159 - val_loss: 0.0159\n",
            "Epoch 85/200\n",
            "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0143 - val_loss: 0.0132\n",
            "Epoch 86/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0129 - val_loss: 0.0125\n",
            "Epoch 87/200\n",
            "10000/10000 [==============================] - 3s 277us/sample - loss: 0.0113 - val_loss: 0.0117\n",
            "Epoch 88/200\n",
            "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0112 - val_loss: 0.0121\n",
            "Epoch 89/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0114 - val_loss: 0.0115\n",
            "Epoch 90/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0104 - val_loss: 0.0108\n",
            "Epoch 91/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0108 - val_loss: 0.0117\n",
            "Epoch 92/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0115 - val_loss: 0.0126\n",
            "Epoch 93/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0115 - val_loss: 0.0127\n",
            "Epoch 94/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0105 - val_loss: 0.0118\n",
            "Epoch 95/200\n",
            "10000/10000 [==============================] - 3s 263us/sample - loss: 0.0100 - val_loss: 0.0106\n",
            "Epoch 96/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0109 - val_loss: 0.0105\n",
            "Epoch 97/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0116 - val_loss: 0.0094\n",
            "Epoch 98/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0112 - val_loss: 0.0118\n",
            "Epoch 99/200\n",
            "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0107 - val_loss: 0.0102\n",
            "Epoch 100/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0107 - val_loss: 0.0119\n",
            "Epoch 101/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0130 - val_loss: 0.0119\n",
            "Epoch 102/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0127 - val_loss: 0.0153\n",
            "Epoch 103/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0115 - val_loss: 0.0118\n",
            "Epoch 104/200\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0107 - val_loss: 0.0107\n",
            "Epoch 105/200\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0104 - val_loss: 0.0107\n",
            "Epoch 106/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0099 - val_loss: 0.0102\n",
            "Epoch 107/200\n",
            "10000/10000 [==============================] - 2s 249us/sample - loss: 0.0101 - val_loss: 0.0107\n",
            "Epoch 108/200\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0091 - val_loss: 0.0100\n",
            "Epoch 109/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0096 - val_loss: 0.0106\n",
            "Epoch 110/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0125 - val_loss: 0.0143\n",
            "Epoch 111/200\n",
            "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0106 - val_loss: 0.0110\n",
            "Epoch 112/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0095 - val_loss: 0.0116\n",
            "Epoch 113/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0101 - val_loss: 0.0119\n",
            "Epoch 114/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0090 - val_loss: 0.0086\n",
            "Epoch 115/200\n",
            "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0083 - val_loss: 0.0099\n",
            "Epoch 116/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0085 - val_loss: 0.0100\n",
            "Epoch 117/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0089 - val_loss: 0.0102\n",
            "Epoch 118/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0087 - val_loss: 0.0084\n",
            "Epoch 119/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0078 - val_loss: 0.0089\n",
            "Epoch 120/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0074 - val_loss: 0.0086\n",
            "Epoch 121/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0077 - val_loss: 0.0090\n",
            "Epoch 122/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0081 - val_loss: 0.0099\n",
            "Epoch 123/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0088 - val_loss: 0.0125\n",
            "Epoch 124/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0091 - val_loss: 0.0106\n",
            "Epoch 125/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0079 - val_loss: 0.0080\n",
            "Epoch 126/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0076 - val_loss: 0.0078\n",
            "Epoch 127/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0078 - val_loss: 0.0077\n",
            "Epoch 128/200\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0080 - val_loss: 0.0072\n",
            "Epoch 129/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0077 - val_loss: 0.0076\n",
            "Epoch 130/200\n",
            "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0065 - val_loss: 0.0076\n",
            "Epoch 131/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0072 - val_loss: 0.0073\n",
            "Epoch 132/200\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0077 - val_loss: 0.0095\n",
            "Epoch 133/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0079 - val_loss: 0.0094\n",
            "Epoch 134/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0086 - val_loss: 0.0083\n",
            "Epoch 135/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0077 - val_loss: 0.0074\n",
            "Epoch 136/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0067 - val_loss: 0.0073\n",
            "Epoch 137/200\n",
            "10000/10000 [==============================] - 3s 263us/sample - loss: 0.0073 - val_loss: 0.0077\n",
            "Epoch 138/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0064 - val_loss: 0.0084\n",
            "Epoch 139/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0067 - val_loss: 0.0079\n",
            "Epoch 140/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0079 - val_loss: 0.0086\n",
            "Epoch 141/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0072 - val_loss: 0.0071\n",
            "Epoch 142/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0070 - val_loss: 0.0076\n",
            "Epoch 143/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0065 - val_loss: 0.0074\n",
            "Epoch 144/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0071 - val_loss: 0.0105\n",
            "Epoch 145/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0081 - val_loss: 0.0124\n",
            "Epoch 146/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0095 - val_loss: 0.0068\n",
            "Epoch 147/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0065 - val_loss: 0.0079\n",
            "Epoch 148/200\n",
            "10000/10000 [==============================] - 3s 278us/sample - loss: 0.0063 - val_loss: 0.0075\n",
            "Epoch 149/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0118 - val_loss: 0.0108\n",
            "Epoch 150/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0095 - val_loss: 0.0098\n",
            "Epoch 151/200\n",
            "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0073 - val_loss: 0.0069\n",
            "Epoch 152/200\n",
            "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0070 - val_loss: 0.0081\n",
            "Epoch 153/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0063 - val_loss: 0.0062\n",
            "Epoch 154/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0058 - val_loss: 0.0066\n",
            "Epoch 155/200\n",
            "10000/10000 [==============================] - 3s 277us/sample - loss: 0.0062 - val_loss: 0.0056\n",
            "Epoch 156/200\n",
            "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0066 - val_loss: 0.0099\n",
            "Epoch 157/200\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0081 - val_loss: 0.0097\n",
            "Epoch 158/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0077 - val_loss: 0.0099\n",
            "Epoch 159/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0126 - val_loss: 0.0132\n",
            "Epoch 160/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0118 - val_loss: 0.0080\n",
            "Epoch 161/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0074 - val_loss: 0.0068\n",
            "Epoch 162/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0067 - val_loss: 0.0079\n",
            "Epoch 163/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0075 - val_loss: 0.0139\n",
            "Epoch 164/200\n",
            "10000/10000 [==============================] - 3s 279us/sample - loss: 0.0091 - val_loss: 0.0082\n",
            "Epoch 165/200\n",
            "10000/10000 [==============================] - 3s 288us/sample - loss: 0.0091 - val_loss: 0.0066\n",
            "Epoch 166/200\n",
            "10000/10000 [==============================] - 3s 277us/sample - loss: 0.0062 - val_loss: 0.0052\n",
            "Epoch 167/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0050 - val_loss: 0.0045\n",
            "Epoch 168/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0047 - val_loss: 0.0052\n",
            "Epoch 169/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0053 - val_loss: 0.0080\n",
            "Epoch 170/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0081 - val_loss: 0.0052\n",
            "Epoch 171/200\n",
            "10000/10000 [==============================] - 3s 283us/sample - loss: 0.0065 - val_loss: 0.0093\n",
            "Epoch 172/200\n",
            "10000/10000 [==============================] - 3s 279us/sample - loss: 0.0060 - val_loss: 0.0056\n",
            "Epoch 173/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0059 - val_loss: 0.0047\n",
            "Epoch 174/200\n",
            "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0052 - val_loss: 0.0076\n",
            "Epoch 175/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0084 - val_loss: 0.0091\n",
            "Epoch 176/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0092 - val_loss: 0.0070\n",
            "Epoch 177/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0275 - val_loss: 0.0722\n",
            "Epoch 178/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0491 - val_loss: 0.0373\n",
            "Epoch 179/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0232 - val_loss: 0.0155\n",
            "Epoch 180/200\n",
            "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0126 - val_loss: 0.0106\n",
            "Epoch 181/200\n",
            "10000/10000 [==============================] - 3s 263us/sample - loss: 0.0091 - val_loss: 0.0079\n",
            "Epoch 182/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0080 - val_loss: 0.0079\n",
            "Epoch 183/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0066 - val_loss: 0.0063\n",
            "Epoch 184/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0054 - val_loss: 0.0069\n",
            "Epoch 185/200\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0054 - val_loss: 0.0060\n",
            "Epoch 186/200\n",
            "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0067 - val_loss: 0.0065\n",
            "Epoch 187/200\n",
            "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0061 - val_loss: 0.0049\n",
            "Epoch 188/200\n",
            "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0060 - val_loss: 0.0066\n",
            "Epoch 189/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0072 - val_loss: 0.0076\n",
            "Epoch 190/200\n",
            "10000/10000 [==============================] - 3s 273us/sample - loss: 0.0068 - val_loss: 0.0068\n",
            "Epoch 191/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0147 - val_loss: 0.0130\n",
            "Epoch 192/200\n",
            "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0111 - val_loss: 0.0075\n",
            "Epoch 193/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0084 - val_loss: 0.0124\n",
            "Epoch 194/200\n",
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0145 - val_loss: 0.0176\n",
            "Epoch 195/200\n",
            "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0145 - val_loss: 0.0137\n",
            "Epoch 196/200\n",
            "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0130 - val_loss: 0.0114\n",
            "Epoch 197/200\n",
            "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0119 - val_loss: 0.0724\n",
            "Epoch 198/200\n",
            "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0366 - val_loss: 0.0440\n",
            "Epoch 199/200\n",
            "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0294 - val_loss: 0.0192\n",
            "Epoch 200/200\n",
            "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0182 - val_loss: 0.0156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcVcyv3V-rAJ",
        "colab_type": "code",
        "outputId": "27446e28-a6e3-45f1-9c43-57667344cc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "plt.plot(history_two_axis.history['loss'])\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error in Control\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history_two_axis.history['val_loss'])\n",
        "plt.title(\"Validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error in Control\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBVLQZRBd4M4",
        "colab_type": "text"
      },
      "source": [
        "## 2. LSTM for learning time-dependent quantum noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4hiTZTheWaq",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Model definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMsuJCzDGPqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(batch_size, rnn_units, stateful=False): \n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.LSTM(\n",
        "           rnn_units,\n",
        "          return_sequences=True,\n",
        "          stateful=stateful,\n",
        "          recurrent_initializer='glorot_uniform',\n",
        "          batch_input_shape=[batch_size, None, 1]),\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "  return model \n",
        "\n",
        "def train_lstm(data, batch_size, rnn_units, epochs, learning_rate):\n",
        "  print('Start training.')\n",
        "  model = build_model(batch_size, rnn_units)\n",
        "\n",
        "  def loss(labels, logits):\n",
        "    return tf.keras.losses.binary_crossentropy(labels, logits, True)\n",
        "\n",
        " \n",
        "  optimizer = tf.keras.optimizers.Adamax( learning_rate) \n",
        "  model.compile(optimizer=optimizer, loss=loss)\n",
        "  model.summary()\n",
        " \n",
        " \n",
        "  model.fit(data, epochs= epochs)\n",
        "  eval_loss = model.evaluate(data)\n",
        "  print( \"final loss = \", eval_loss)\n",
        "  return model\n",
        "\n",
        "\n",
        "def sample_eval(weights,  eval_samples, epoch, rnn_units, input_lenth):\n",
        "  model = build_model( batch_size, rnn_units, True)\n",
        "  model.build(tf.TensorShape([ batch_size, None, 1]))\n",
        "  model.set_weights(weights)\n",
        "  # Whole sequence sampling and fidelity\n",
        "  eval_samples = eval_samples //  batch_size *  batch_size\n",
        "  sample_data = np.zeros(((eval_samples,  input_length)), np.float64)\n",
        "  sample_n = 0\n",
        "  model.summary()\n",
        "\n",
        "  while sample_n < eval_samples:\n",
        "    model.reset_states()\n",
        "    input_eval = tf.zeros([ batch_size, 1, 1])\n",
        "    output_eval = tf.reshape(model(input_eval), [ batch_size])\n",
        "    output_prob = 1 / (1 + np.exp(-output_eval.numpy()))\n",
        "    sample_data[sample_n:sample_n +  batch_size,\n",
        "                0] =output_prob\n",
        "    for i in range( input_length - 1):\n",
        "      input_eval = tf.cast(\n",
        "          tf.reshape(sample_data[sample_n:sample_n +  batch_size, i],\n",
        "                     [ batch_size, 1, 1]), tf.float32)\n",
        "      output_eval = tf.reshape(model(input_eval), [ batch_size])\n",
        "      output_prob = 1 / (1 + np.exp(-output_eval.numpy()))\n",
        "      sample_data[sample_n:sample_n +  batch_size,\n",
        "                  i + 1] = np.random.binomial(1, output_prob)\n",
        "    sample_n +=  batch_size\n",
        " \n",
        "\n",
        "\n",
        "def generate_data(data_time, data_length, omega_0, exponent, alpha):\n",
        "\n",
        "  timesteps = np.linspace(0.02, data_time, data_length)\n",
        "  q = cirq.GridQubit(0, 0)\n",
        "  phase_s = sympy.symbols(\"phaseshift\")\n",
        "  circuit = cirq.Circuit(cirq.H(q), cirq.Rz(phase_s)(q))\n",
        "  ops = [cirq.X(q)]\n",
        "\n",
        "  params = []\n",
        "  outputs = np.zeros(data_length)\n",
        "\n",
        "  for i in range(data_length):\n",
        "    phaseshift = timesteps[i] * omega_0 + alpha * timesteps[i] ** (exponent +1) / (exponent +1)\n",
        "    expectations = tfq.layers.Expectation()(\n",
        "        circuit,\n",
        "        symbol_names=[phase_s],\n",
        "        symbol_values=[[phaseshift]],\n",
        "        operators=ops \n",
        "    ).numpy().tolist()[0]\n",
        "     \n",
        "    outputs[i]= expectations[0] \n",
        "  return outputs\n",
        "     \n",
        "def load_data(data_size, data_time, data_length, alpha_min, alpha_max, omega_0, exponent):\n",
        "  alpha_list = np.linspace(alpha_min, alpha_max, data_size)\n",
        "  train_data = []\n",
        "  for k in range(data_size):\n",
        "    data1 = generate_data(data_time, data_length, omega_0, exponent, alpha_list[k])\n",
        "    train_data.append(data1)\n",
        "  return np.array(train_data)\n",
        "\n",
        "   \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCbdEZ4qeDty",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Generate training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFLQEJ4r9uP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 30\n",
        "batch_size = 60\n",
        "learning_rate = 0.001\n",
        "\n",
        "rnn_units = 256\n",
        "data_size = 500\n",
        "alpha_min = 0.031\n",
        "alpha_max = 0.2\n",
        "exponent = 0.5\n",
        "omega_0 = 0.7 \n",
        "data_time = 0.5 / alpha_min\n",
        "data_length = 40\n",
        " \n",
        "\n",
        "train_data = load_data(data_size, data_time, data_length, alpha_min, alpha_max, omega_0, exponent)  # this should be a numpy array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBNZYAO6edop",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Training the LSTM model on the time-dependent expectation values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikQ6MXZaec4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "\n",
        "def build_example(chunk):\n",
        "  input_seq = tf.cast(tf.concat([[0], chunk], 0), tf.float32)\n",
        "  target = tf.concat([chunk, [0]], 0)\n",
        "  input_seq = input_seq[:-1]\n",
        "  target = target[:-1]\n",
        "  return tf.expand_dims(input_seq, 1), tf.expand_dims(target, 1)\n",
        "BUFFER_SIZE = 100\n",
        "data = data.map(build_example).shuffle(BUFFER_SIZE).batch(\n",
        "    batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89_k81SYxKFt",
        "colab_type": "code",
        "outputId": "3e2f80b8-f222-41d1-fd22-56e66ad6a291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = train_lstm(data, batch_size, rnn_units, epoch, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (60, None, 256)           264192    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (60, None, 1)             257       \n",
            "=================================================================\n",
            "Total params: 264,449\n",
            "Trainable params: 264,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train for 8 steps\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 3s 322ms/step - loss: 0.6305\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 1s 159ms/step - loss: 0.2761\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 0.1193\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.0911\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.0797\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 0.0733\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 0.0689\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 0.0656\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 0.0629\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.0605\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 0.0584\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 0.0564\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 0.0546\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 0.0530\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 0.0514\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 0.0500\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.0486\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 0.0473\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 0.0461\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 0.0449\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 1s 146ms/step - loss: 0.0438\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 0.0427\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 0.0417\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 1s 146ms/step - loss: 0.0407\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 0.0398\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 1s 146ms/step - loss: 0.0389\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.0381\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 0.0372\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 0.0365\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 0.0357\n",
            "8/8 [==============================] - 1s 101ms/step - loss: 0.0353\n",
            "final loss =  0.03528309985995293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3a12DaDnKBg",
        "colab_type": "code",
        "outputId": "2082743d-0129-4210-8275-187c61b1628d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(model.history.history['loss']) \n",
        "plt.xlabel(\"Training Epochs\", fontsize='14')\n",
        "plt.ylabel(\"Error in LSTM samples\", fontsize='14')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEKCAYAAAD5MJl4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngZODmZlN4ORgZmYTODmYmdkE/x8R2+xNr3AFSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}