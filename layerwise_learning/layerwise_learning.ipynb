{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layerwise_learning_copy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFq2aRw_w3cL",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Quantum Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOzjTj_JxBnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdEtQYqo32Ej",
        "colab_type": "text"
      },
      "source": [
        "# Layerwise learning for quantum neural networks\n",
        "\n",
        "Author : Andrea Skolik\n",
        "\n",
        "Contributors : Masoud Mohseni\n",
        "\n",
        "Created : 2019\n",
        "\n",
        "Last updated : 2020-Feb-27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8hXbFbkv_D_",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/quantum/blob/research/layerwise_learning/layerwise_learning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nKBqagSxl6N",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will use the technique introduced in [1] to efficiently train a quantum neural network without making any initial guesses about the structure that's neccessary to solve a certain learning task. To do this, we successively add layers to a QNN during training, which does not only make training faster, but also ensures a better signal-to-noise ratio compared to training the full circuit when done on real hardware.\n",
        "\n",
        "It is well known that randomly initialized parametrized quantum circuits suffer from exponentially decaying gradients as circuits grow in size [2]. \n",
        "One strategy to avoid this is finding clever initialization schemes for deep circuits. Another approach which we take here instead focuses on the structure of the circuit, and shows how a deep parametrized circuit can be constructed during training. By training individual partitions of the circuit as it grows, we avoid the randomization effect that causes barren plateaus. This is mainly of importance on noisy intermediate-scale quantum (NISQ) devices, as these will suffer most from the unfavorable signal-to-noise ratio when running variational algorithms. As the gradients produced by circuits grow smaller, we need more and more measurements from a quantum device to accurately estimate them. When using layerwise learning (LL), gradients stay larger during training and we therefore need less measurements to get sufficient training signal for the optimizer. Additionally, we decrease the overall number of parameter updates, so that LL provides an efficient strategy to run variational algorithms on NISQ devices.\n",
        "\n",
        "LL works in two phases as shown in the figure below:\n",
        "\n",
        "![Two phases of layerwise learning](https://github.com/tensorflow/quantum/blob/research/layerwise_learning/images/layers.png)\n",
        "\n",
        "In the first phase, we start training with a small number of layers and train those for a fixed number of epochs. After that, we add another set of layers and freeze the parameters of the previous step's layers. We repeat this process until the desired depth is reached. In phase two, we perform additional optimization sweeps over larger subsets of the layers using the final circuit configuration from phase one. The parameters from this circuit give us a good starting point to optimize quarters, halves, or even the full circuit without initializing on a barren plateau.\n",
        "\n",
        "This kind of learning scheme can be used for various types of learning tasks and input data, so long as the QNN structure allows iteratively building the circuits. In this notebook we look at a simple example of classifying MNIST digits with randomly generated layers.\n",
        "\n",
        "\n",
        "\n",
        "[1] Layerwise learning for quantum neural networks,  A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib, in preparation.\n",
        "\n",
        "[2] Barren plateaus in quantum neural network training landscapes,   J.  R.  McClean,  S.  Boixo,  V.  N.  Smelyanskiy,  R.  Babbush,  and H. Neven, Nature Communications 9 (2018)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqx89K2caCR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade cirq==0.7.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuXxC5fbaGAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyrqkto1aHQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tfq-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW1LRRSuY935",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import tensorflow_quantum as tfq\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNVb3cr2QMPZ",
        "colab_type": "text"
      },
      "source": [
        "First, we need to create the layers we want to use in our circuit. We construct layers that apply a randomly chosen X, Y, or Z gate on each qubit, and a ladder of CZ gates that connect them. This is the same structure as used in [2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gChRsDMxZARo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_layer(qubits, layer_id):\n",
        "    symbols = [sympy.Symbol(layer_id + '-' + str(i)) for i in range(len(qubits))]\n",
        "    gate_set = [cirq.Rx, cirq.Ry, cirq.Rz]\n",
        "    gates = [random.choice(gate_set)(symbols[i])(q) for i, q in enumerate(qubits)]\n",
        "\n",
        "    for control, target in zip(qubits, qubits[1:]):\n",
        "        gates.append(cirq.CZ(control, target))\n",
        "\n",
        "    return gates, symbols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2B3UfdrQ4Cx",
        "colab_type": "text"
      },
      "source": [
        "We also need to prepare the training data. For simplicity, we borrow the training data and data input scheme from the MNIST classification example in the TFQ docs [TODO: add link to TFQ notebook]. Namely we downsample and flatten the images, such that we have vectors with binary entries. These bitstrings are then fed to the circuit by applying a layer of X gates to qubits that correspond to ones in the image vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhhlWTikZ4qC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_image(x):\n",
        "    x = tf.reshape(x, [1, 28, 28, 1])\n",
        "    x = tf.image.resize(x, [4, 4])\n",
        "    x = tf.reshape(x, [4, 4])\n",
        "    x = x / 255\n",
        "    return x.numpy()\n",
        "\n",
        "def remove_contradicting(xs, ys):\n",
        "    mapping = collections.defaultdict(set)\n",
        "    for x, y in zip(xs, ys):\n",
        "        mapping[str(x)].add(y)\n",
        "\n",
        "    return zip(*((x, y) for x, y in zip(xs, ys) if len(mapping[str(x)]) == 1))\n",
        "\n",
        "def convert_to_circuit(image):\n",
        "    values = np.ndarray.flatten(image)\n",
        "    qubits = cirq.GridQubit.rect(1, len(values))\n",
        "    circuit = cirq.Circuit()\n",
        "\n",
        "    for i, value in enumerate(values):\n",
        "        if value > 0.5:\n",
        "            circuit.append(cirq.X(qubits[i]))\n",
        "\n",
        "    return circuit\n",
        "\n",
        "def convert_label(y):\n",
        "    if y == 3:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return -1.0\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_train))\n",
        "\n",
        "x_train, y_train = zip(*((x, y) for x, y in zip(x_train, y_train) if y in [3, 6]))\n",
        "x_test, y_test = zip(*((x, y) for x, y in zip(x_test, y_test) if y in [3, 6]))\n",
        "\n",
        "x_train = [reduce_image(x) for x in x_train]\n",
        "x_test = [reduce_image(x) for x in x_test]\n",
        "\n",
        "x_train, y_train = remove_contradicting(x_train, y_train)\n",
        "x_test, y_test = remove_contradicting(x_test, y_test)\n",
        "\n",
        "print(\"Number of filtered training examples:\", len(x_train))\n",
        "print(\"Number of filtered test examples:\", len(x_test))\n",
        "\n",
        "x_train = [convert_to_circuit(x) for x in x_train]\n",
        "x_test = [convert_to_circuit(x) for x in x_test]\n",
        "\n",
        "y_train = [convert_label(y) for y in y_train]\n",
        "y_test = [convert_label(y) for y in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHoO_pDUVKTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# increase for more accurate results\n",
        "NUM_EXAMPLES = 128\n",
        "x_train = x_train[:NUM_EXAMPLES]\n",
        "y_train = y_train[:NUM_EXAMPLES]\n",
        "\n",
        "x_train = tfq.convert_to_tensor(x_train)\n",
        "x_test = tfq.convert_to_tensor(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBnZIePfTDU4",
        "colab_type": "text"
      },
      "source": [
        "Now we will set up our training loop. We specify the number of qubits in the circuit, how many layer addition steps to perform, and how many layers to add in each step. The latter is a hyperparameter of our model that can be tuned for the learning task at hand. There is a trade-off between keeping the trained partitions as small as possible, but at the same time not too small to make significant progress on the learning task. You can play with the hyperparameters below to notice this difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q10lo6OmTNgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_qubits = 6\n",
        "n_layer_steps = 5\n",
        "n_layers_to_add = 2\n",
        "data_qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "readout = cirq.GridQubit(0, n_qubits-1)\n",
        "\n",
        "symbols = []\n",
        "layers = []\n",
        "weights = []\n",
        "\n",
        "training_history = []\n",
        "\n",
        "for layer_id in range(n_layer_steps):\n",
        "    print(\"\\nLayer:\", layer_id)\n",
        "    circuit = cirq.Circuit()\n",
        "    for i in range(n_layers_to_add):\n",
        "        layer, layer_symbols = create_layer(data_qubits, f'layer_{layer_id}_{i}')\n",
        "        layers.append(layer)\n",
        "        symbols.append(layer_symbols)\n",
        "\n",
        "    circuit += layers\n",
        "\n",
        "    # prepare the readout qubit\n",
        "    circuit.append(cirq.X(readout))\n",
        "    circuit.append(cirq.H(readout))\n",
        "    circuit.append(cirq.X(readout))\n",
        "    readout_op = cirq.Z(readout)\n",
        "\n",
        "    # setup the Keras model\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(), dtype=tf.dtypes.string))\n",
        "    model.add(\n",
        "        tfq.layers.PQC(\n",
        "            model_circuit=circuit,\n",
        "            operators=readout_op,\n",
        "            differentiator=tfq.differentiators.ParameterShift(),\n",
        "            initializer=tf.keras.initializers.Zeros))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.squared_hinge,\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
        "\n",
        "    # set parameters to 0 for new layers\n",
        "    model.set_weights([np.pad(weights, (n_qubits*n_layers_to_add, 0))])\n",
        "\n",
        "    model.fit(x_train,\n",
        "              y_train,\n",
        "              batch_size=128,\n",
        "              epochs=20,\n",
        "              verbose=2,\n",
        "              validation_data=(x_test, y_test))\n",
        "\n",
        "    qnn_results = model.evaluate(x_test, y_test)\n",
        "    training_history.append(qnn_results)\n",
        "\n",
        "    weights = model.get_weights()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTDPaUmNXhhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(training_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX2Ln6dfUb47",
        "colab_type": "text"
      },
      "source": [
        "As already pointed out in the MNIST example notebook, a classical neural network is hard to beat on a simple learning task like this, especially with a basic data encoding scheme as used above. In general, layerwise learning can be used in arbitrary configurations that allow successively stacking and training layers, and it is independent of the data encoding scheme used - so feel free to play with more elaborate data sets as well!"
      ]
    }
  ]
}