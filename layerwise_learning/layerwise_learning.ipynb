{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Layerwise_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdEtQYqo32Ej",
        "colab_type": "text"
      },
      "source": [
        "# Layerwise learning for quantum neural networks\n",
        "\n",
        "In this notebook, we will use the technique introduced in [1] to efficiently train a quantum neural network without making any initial guesses about the structure that's neccessary to solve a certain learning task. To do this, we successively add layers to a QNN during training, which does not only make training faster, but also ensures a better signal-to-noise ratio compared to training the full circuit when done on real hardware.\n",
        "\n",
        "It is well known that randomly initialized parametrized quantum circuits suffer from exponentially decaying gradients as circuits grow in size [2]. \n",
        "One strategy to avoid this is finding clever initialization schemes for deep circuits. Another approach which we take here instead focuses on the structure of the circuit, and shows how a deep parametrized circuit can be constructed during training. By training individual partitions of the circuit as it grows, we avoid the randomization effect that causes barren plateaus. This is mainly of importance on noisy intermediate-scale quantum (NISQ) devices, as these will suffer most from the unfavorable signal-to-noise ratio when running variational algorithms. As the gradients produced by circuits grow smaller, we need more and more measurements from a quantum device to accurately estimate them. When using layerwise learning (LL), gradients stay larger during training and we therefore need less measurements to get sufficient training signal for the optimizer. Additionally, we decrease the overall number of parameter updates, so that LL provides an efficient strategy to run variational algorithms on NISQ devices.\n",
        "\n",
        "LL works in two phases as shown in the figure below:\n",
        "\n",
        "![Two phases of layerwise learning](/content/layers.png)\n",
        "\n",
        "In the first phase, we start training with a small number of layers and train those for a fixed number of epochs. After that, we add another set of layers and freeze the parameters of the previous step's layers. We repeat this process until the desired depth is reached. In phase two, we perform additional optimization sweeps over larger subsets of the layers using the final circuit configuration from phase one. The parameters from this circuit give us a good starting point to optimize quarters, halves, or even the full circuit without initializing on a barren plateau.\n",
        "\n",
        "This kind of learning scheme can be used for various types of learning tasks and input data, so long as the QNN structure allows iteratively building the circuits. In this notebook we look at a simple example of classifying MNIST digits with randomly generated layers.\n",
        "\n",
        "\n",
        "\n",
        "[1] Layerwise learning for quantum neural networks,  A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib, in preparation.\n",
        "\n",
        "[2] Barren plateaus in quantum neural network training landscapes,   J.  R.  McClean,  S.  Boixo,  V.  N.  Smelyanskiy,  R.  Babbush,  and H. Neven, Nature Communications 9 (2018)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eVDbG_2ZhMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade pip\n",
        "!pip install cirq==0.6.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2UaAnxnY6Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQahV2RMY72p",
        "colab_type": "code",
        "outputId": "6323ce4f-6e5a-442b-84f2-be9958dcf87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "h = \"2dfcfceb9726fa73c40381c037dc01facd3d061e\"\n",
        "!cd ~/\n",
        "!rm -r -f TFQuantum/\n",
        "!git clone https://{h}:{h}@github.com/quantumlib/TFQuantum.git;\n",
        "!pip install --upgrade ./TFQuantum/wheels/tfquantum-0.2.0-cp36-cp36m-manylinux1_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TFQuantum'...\n",
            "remote: Enumerating objects: 301, done.\u001b[K\n",
            "remote: Counting objects: 100% (301/301), done.\u001b[K\n",
            "remote: Compressing objects: 100% (238/238), done.\u001b[K\n",
            "remote: Total 18121 (delta 177), reused 131 (delta 63), pack-reused 17820\u001b[K\n",
            "Receiving objects: 100% (18121/18121), 107.51 MiB | 14.04 MiB/s, done.\n",
            "Resolving deltas: 100% (12447/12447), done.\n",
            "Processing ./TFQuantum/wheels/tfquantum-0.2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: cirq==0.6.0 in /usr/local/lib/python3.6/dist-packages (from tfquantum==0.2.0) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: sympy in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (0.25.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.16 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: requests~=2.18 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx==2.3 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (3.6.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-api-python-client~=1.6 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (1.7.11)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib~=3.0 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (3.1.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf==3.8.0 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (3.8.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers~=2.0 in /usr/local/lib/python3.6/dist-packages (from cirq==0.6.0->tfquantum==0.2.0) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->cirq==0.6.0->tfquantum==0.2.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->cirq==0.6.0->tfquantum==0.2.0) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->cirq==0.6.0->tfquantum==0.2.0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.18->cirq==0.6.0->tfquantum==0.2.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests~=2.18->cirq==0.6.0->tfquantum==0.2.0) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.18->cirq==0.6.0->tfquantum==0.2.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests~=2.18->cirq==0.6.0->tfquantum==0.2.0) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.3->cirq==0.6.0->tfquantum==0.2.0) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (1.11.1)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->cirq==0.6.0->tfquantum==0.2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->cirq==0.6.0->tfquantum==0.2.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->cirq==0.6.0->tfquantum==0.2.0) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.8.0->cirq==0.6.0->tfquantum==0.2.0) (45.1.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.4.1->google-api-python-client~=1.6->cirq==0.6.0->tfquantum==0.2.0) (0.4.8)\n",
            "Installing collected packages: tfquantum\n",
            "Successfully installed tfquantum-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW1LRRSuY935",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import tensorflow_quantum as tfq\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNVb3cr2QMPZ",
        "colab_type": "text"
      },
      "source": [
        "First, we need to create the layers we want to use in our circuit. We construct layers that apply a randomly chosen X, Y, or Z gate on each qubit, and a ladder of CZ gates that connect them. This is the same structure as used in [2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gChRsDMxZARo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_layer(qubits, layer_id):\n",
        "    symbols = [sympy.Symbol(layer_id + '-' + str(i)) for i in range(len(qubits))]\n",
        "    gate_set = [cirq.Rx, cirq.Ry, cirq.Rz]\n",
        "    gates = [random.choice(gate_set)(symbols[i])(q) for i, q in enumerate(qubits)]\n",
        "\n",
        "    for control, target in zip(qubits, qubits[1:]):\n",
        "        gates.append(cirq.CZ(control, target))\n",
        "\n",
        "    return gates, symbols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2B3UfdrQ4Cx",
        "colab_type": "text"
      },
      "source": [
        "We also need to prepare the training data. For simplicity, we borrow the training data and data input scheme from the MNIST classification example in the TFQ docs [TODO: add link to TFQ notebook]. Namely we downsample and flatten the images, such that we have vectors with binary entries. These bitstrings are then fed to the circuit by applying a layer of X gates to qubits that correspond to ones in the image vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhhlWTikZ4qC",
        "colab_type": "code",
        "outputId": "16083824-7321-43a2-dd54-2fed678cd13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def reduce_image(x):\n",
        "    x = tf.reshape(x, [1, 28, 28, 1])\n",
        "    x = tf.image.resize(x, [4, 4])\n",
        "    x = tf.reshape(x, [4, 4])\n",
        "    x = x / 255\n",
        "    return x.numpy()\n",
        "\n",
        "def remove_contradicting(xs, ys):\n",
        "    mapping = collections.defaultdict(set)\n",
        "    for x, y in zip(xs, ys):\n",
        "        mapping[str(x)].add(y)\n",
        "\n",
        "    return zip(*((x, y) for x, y in zip(xs, ys) if len(mapping[str(x)]) == 1))\n",
        "\n",
        "def convert_to_circuit(image):\n",
        "    values = np.ndarray.flatten(image)\n",
        "    qubits = cirq.GridQubit.rect(1, len(values))\n",
        "    circuit = cirq.Circuit()\n",
        "\n",
        "    for i, value in enumerate(values):\n",
        "        if value > 0.5:\n",
        "            circuit.append(cirq.X(qubits[i]))\n",
        "\n",
        "    return circuit\n",
        "\n",
        "def convert_label(y):\n",
        "    if y == 3:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return -1.0\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"Number of original training examples:\", len(x_train))\n",
        "print(\"Number of original test examples:\", len(x_train))\n",
        "\n",
        "x_train, y_train = zip(*((x, y) for x, y in zip(x_train, y_train) if y in [3, 6]))\n",
        "x_test, y_test = zip(*((x, y) for x, y in zip(x_test, y_test) if y in [3, 6]))\n",
        "\n",
        "x_train = [reduce_image(x) for x in x_train]\n",
        "x_test = [reduce_image(x) for x in x_test]\n",
        "\n",
        "x_train, y_train = remove_contradicting(x_train, y_train)\n",
        "x_test, y_test = remove_contradicting(x_test, y_test)\n",
        "\n",
        "print(\"Number of filtered training examples:\", len(x_train))\n",
        "print(\"Number of filtered test examples:\", len(x_test))\n",
        "\n",
        "x_train = [convert_to_circuit(x) for x in x_train]\n",
        "x_test = [convert_to_circuit(x) for x in x_test]\n",
        "\n",
        "y_train = [convert_label(y) for y in y_train]\n",
        "y_test = [convert_label(y) for y in y_test]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of original training examples: 60000\n",
            "Number of original test examples: 60000\n",
            "Number of filtered training examples: 11520\n",
            "Number of filtered test examples: 1906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHoO_pDUVKTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# increase for more accurate results\n",
        "NUM_EXAMPLES = 128\n",
        "x_train = x_train[:NUM_EXAMPLES]\n",
        "y_train = y_train[:NUM_EXAMPLES]\n",
        "\n",
        "x_train = tfq.convert_to_tensor(x_train)\n",
        "x_test = tfq.convert_to_tensor(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBnZIePfTDU4",
        "colab_type": "text"
      },
      "source": [
        "Now we will set up our training loop. We specify the number of qubits in the circuit, how many layer addition steps to perform, and how many layers to add in each step. The latter is a hyperparameter of our model that can be tuned for the learning task at hand. There is a trade-off between keeping the trained partitions as small as possible, but at the same time not too small to make significant progress on the learning task. You can play with the hyperparameters below to notice this difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q10lo6OmTNgp",
        "colab_type": "code",
        "outputId": "89acc8e4-0fbc-4f0e-d383-eef8e5aa7585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_qubits = 6\n",
        "n_layer_steps = 5\n",
        "n_layers_to_add = 2\n",
        "data_qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "readout = cirq.GridQubit(0, n_qubits-1)\n",
        "\n",
        "symbols = []\n",
        "layers = []\n",
        "weights = []\n",
        "\n",
        "training_history = []\n",
        "\n",
        "for layer_id in range(n_layer_steps):\n",
        "    print(\"\\nLayer:\", layer_id)\n",
        "    circuit = cirq.Circuit()\n",
        "    for i in range(n_layers_to_add):\n",
        "        layer, layer_symbols = create_layer(data_qubits, f'layer_{layer_id}_{i}')\n",
        "        layers.append(layer)\n",
        "        symbols.append(layer_symbols)\n",
        "\n",
        "    circuit += layers\n",
        "\n",
        "    # prepare the readout qubit\n",
        "    circuit.append(cirq.X(readout))\n",
        "    circuit.append(cirq.H(readout))\n",
        "    circuit.append(cirq.X(readout))\n",
        "    readout_op = cirq.Z(readout)\n",
        "\n",
        "    # setup the Keras model\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(), dtype=tf.dtypes.string))\n",
        "    model.add(\n",
        "        tfq.layers.PQC(\n",
        "            model_circuit=circuit,\n",
        "            operators=readout_op,\n",
        "            differentiator=tfq.differentiators.ParameterShift(),\n",
        "            initializer=tf.keras.initializers.Zeros))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.squared_hinge,\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
        "\n",
        "    # set parameters to 0 for new layers\n",
        "    model.set_weights([np.pad(weights, (n_qubits*n_layers_to_add, 0))])\n",
        "\n",
        "    model.fit(x_train,\n",
        "              y_train,\n",
        "              batch_size=128,\n",
        "              epochs=20,\n",
        "              verbose=2,\n",
        "              validation_data=(x_test, y_test))\n",
        "\n",
        "    qnn_results = model.evaluate(x_test, y_test)\n",
        "    training_history.append(qnn_results)\n",
        "\n",
        "    weights = model.get_weights()[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Layer: 0\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "pqc_20 (PQC)                 (None, 1)                 12        \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 128 samples, validate on 1906 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75daa20ac8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "WARNING: AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75daa20ac8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "128/128 - 2s - loss: 1.0000 - val_loss: 1.0000\n",
            "Epoch 2/20\n",
            "128/128 - 2s - loss: 1.0000 - val_loss: 1.0000\n",
            "Epoch 3/20\n",
            "128/128 - 2s - loss: 1.0000 - val_loss: 0.9998\n",
            "Epoch 4/20\n",
            "128/128 - 2s - loss: 0.9999 - val_loss: 0.9996\n",
            "Epoch 5/20\n",
            "128/128 - 2s - loss: 0.9998 - val_loss: 0.9992\n",
            "Epoch 6/20\n",
            "128/128 - 2s - loss: 0.9996 - val_loss: 0.9987\n",
            "Epoch 7/20\n",
            "128/128 - 2s - loss: 0.9993 - val_loss: 0.9981\n",
            "Epoch 8/20\n",
            "128/128 - 2s - loss: 0.9989 - val_loss: 0.9973\n",
            "Epoch 9/20\n",
            "128/128 - 2s - loss: 0.9984 - val_loss: 0.9963\n",
            "Epoch 10/20\n",
            "128/128 - 2s - loss: 0.9979 - val_loss: 0.9951\n",
            "Epoch 11/20\n",
            "128/128 - 2s - loss: 0.9972 - val_loss: 0.9938\n",
            "Epoch 12/20\n",
            "128/128 - 2s - loss: 0.9964 - val_loss: 0.9922\n",
            "Epoch 13/20\n",
            "128/128 - 2s - loss: 0.9955 - val_loss: 0.9905\n",
            "Epoch 14/20\n",
            "128/128 - 2s - loss: 0.9946 - val_loss: 0.9886\n",
            "Epoch 15/20\n",
            "128/128 - 2s - loss: 0.9935 - val_loss: 0.9864\n",
            "Epoch 16/20\n",
            "128/128 - 2s - loss: 0.9923 - val_loss: 0.9841\n",
            "Epoch 17/20\n",
            "128/128 - 2s - loss: 0.9909 - val_loss: 0.9815\n",
            "Epoch 18/20\n",
            "128/128 - 2s - loss: 0.9895 - val_loss: 0.9787\n",
            "Epoch 19/20\n",
            "128/128 - 2s - loss: 0.9880 - val_loss: 0.9758\n",
            "Epoch 20/20\n",
            "128/128 - 2s - loss: 0.9863 - val_loss: 0.9726\n",
            "1906/1906 [==============================] - 1s 491us/sample - loss: 0.9726\n",
            "\n",
            "Layer: 1\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "pqc_21 (PQC)                 (None, 1)                 24        \n",
            "=================================================================\n",
            "Total params: 24\n",
            "Trainable params: 24\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 128 samples, validate on 1906 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75dd2acef0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "WARNING: AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75dd2acef0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "128/128 - 5s - loss: 1.1210 - val_loss: 1.1716\n",
            "Epoch 2/20\n",
            "128/128 - 4s - loss: 1.1109 - val_loss: 1.1571\n",
            "Epoch 3/20\n",
            "128/128 - 4s - loss: 1.1009 - val_loss: 1.1424\n",
            "Epoch 4/20\n",
            "128/128 - 4s - loss: 1.0908 - val_loss: 1.1276\n",
            "Epoch 5/20\n",
            "128/128 - 4s - loss: 1.0808 - val_loss: 1.1128\n",
            "Epoch 6/20\n",
            "128/128 - 4s - loss: 1.0708 - val_loss: 1.0980\n",
            "Epoch 7/20\n",
            "128/128 - 4s - loss: 1.0610 - val_loss: 1.0832\n",
            "Epoch 8/20\n",
            "128/128 - 4s - loss: 1.0513 - val_loss: 1.0685\n",
            "Epoch 9/20\n",
            "128/128 - 4s - loss: 1.0419 - val_loss: 1.0539\n",
            "Epoch 10/20\n",
            "128/128 - 4s - loss: 1.0326 - val_loss: 1.0396\n",
            "Epoch 11/20\n",
            "128/128 - 4s - loss: 1.0237 - val_loss: 1.0254\n",
            "Epoch 12/20\n",
            "128/128 - 4s - loss: 1.0150 - val_loss: 1.0115\n",
            "Epoch 13/20\n",
            "128/128 - 4s - loss: 1.0067 - val_loss: 0.9979\n",
            "Epoch 14/20\n",
            "128/128 - 4s - loss: 0.9988 - val_loss: 0.9847\n",
            "Epoch 15/20\n",
            "128/128 - 4s - loss: 0.9913 - val_loss: 0.9719\n",
            "Epoch 16/20\n",
            "128/128 - 4s - loss: 0.9842 - val_loss: 0.9595\n",
            "Epoch 17/20\n",
            "128/128 - 4s - loss: 0.9776 - val_loss: 0.9476\n",
            "Epoch 18/20\n",
            "128/128 - 4s - loss: 0.9715 - val_loss: 0.9362\n",
            "Epoch 19/20\n",
            "128/128 - 4s - loss: 0.9659 - val_loss: 0.9253\n",
            "Epoch 20/20\n",
            "128/128 - 4s - loss: 0.9608 - val_loss: 0.9150\n",
            "1906/1906 [==============================] - 2s 870us/sample - loss: 0.9150\n",
            "\n",
            "Layer: 2\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "pqc_22 (PQC)                 (None, 1)                 36        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 128 samples, validate on 1906 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75da8d9f98>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "WARNING: AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75da8d9f98>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "128/128 - 9s - loss: 1.1365 - val_loss: 1.1929\n",
            "Epoch 2/20\n",
            "128/128 - 8s - loss: 1.1258 - val_loss: 1.1777\n",
            "Epoch 3/20\n",
            "128/128 - 8s - loss: 1.1151 - val_loss: 1.1624\n",
            "Epoch 4/20\n",
            "128/128 - 8s - loss: 1.1045 - val_loss: 1.1470\n",
            "Epoch 5/20\n",
            "128/128 - 8s - loss: 1.0940 - val_loss: 1.1317\n",
            "Epoch 6/20\n",
            "128/128 - 8s - loss: 1.0835 - val_loss: 1.1163\n",
            "Epoch 7/20\n",
            "128/128 - 8s - loss: 1.0732 - val_loss: 1.1011\n",
            "Epoch 8/20\n",
            "128/128 - 8s - loss: 1.0630 - val_loss: 1.0859\n",
            "Epoch 9/20\n",
            "128/128 - 8s - loss: 1.0531 - val_loss: 1.0709\n",
            "Epoch 10/20\n",
            "128/128 - 8s - loss: 1.0434 - val_loss: 1.0561\n",
            "Epoch 11/20\n",
            "128/128 - 8s - loss: 1.0340 - val_loss: 1.0415\n",
            "Epoch 12/20\n",
            "128/128 - 8s - loss: 1.0249 - val_loss: 1.0272\n",
            "Epoch 13/20\n",
            "128/128 - 8s - loss: 1.0161 - val_loss: 1.0132\n",
            "Epoch 14/20\n",
            "128/128 - 8s - loss: 1.0078 - val_loss: 0.9996\n",
            "Epoch 15/20\n",
            "128/128 - 8s - loss: 0.9998 - val_loss: 0.9864\n",
            "Epoch 16/20\n",
            "128/128 - 8s - loss: 0.9922 - val_loss: 0.9736\n",
            "Epoch 17/20\n",
            "128/128 - 8s - loss: 0.9852 - val_loss: 0.9613\n",
            "Epoch 18/20\n",
            "128/128 - 8s - loss: 0.9786 - val_loss: 0.9494\n",
            "Epoch 19/20\n",
            "128/128 - 8s - loss: 0.9724 - val_loss: 0.9381\n",
            "Epoch 20/20\n",
            "128/128 - 8s - loss: 0.9668 - val_loss: 0.9273\n",
            "1906/1906 [==============================] - 2s 1ms/sample - loss: 0.9273\n",
            "\n",
            "Layer: 3\n",
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "pqc_23 (PQC)                 (None, 1)                 48        \n",
            "=================================================================\n",
            "Total params: 48\n",
            "Trainable params: 48\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 128 samples, validate on 1906 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75da5c8eb8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "WARNING: AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75da5c8eb8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "128/128 - 14s - loss: 0.9575 - val_loss: 0.8886\n",
            "Epoch 2/20\n",
            "128/128 - 13s - loss: 0.9518 - val_loss: 0.8752\n",
            "Epoch 3/20\n",
            "128/128 - 13s - loss: 0.9476 - val_loss: 0.8637\n",
            "Epoch 4/20\n",
            "128/128 - 13s - loss: 0.9448 - val_loss: 0.8544\n",
            "Epoch 5/20\n",
            "128/128 - 13s - loss: 0.9432 - val_loss: 0.8472\n",
            "Epoch 6/20\n",
            "128/128 - 13s - loss: 0.9425 - val_loss: 0.8421\n",
            "Epoch 7/20\n",
            "128/128 - 13s - loss: 0.9423 - val_loss: 0.8389\n",
            "Epoch 8/20\n",
            "128/128 - 13s - loss: 0.9421 - val_loss: 0.8372\n",
            "Epoch 9/20\n",
            "128/128 - 13s - loss: 0.9418 - val_loss: 0.8368\n",
            "Epoch 10/20\n",
            "128/128 - 13s - loss: 0.9413 - val_loss: 0.8373\n",
            "Epoch 11/20\n",
            "128/128 - 13s - loss: 0.9405 - val_loss: 0.8387\n",
            "Epoch 12/20\n",
            "128/128 - 13s - loss: 0.9396 - val_loss: 0.8406\n",
            "Epoch 13/20\n",
            "128/128 - 13s - loss: 0.9387 - val_loss: 0.8430\n",
            "Epoch 14/20\n",
            "128/128 - 13s - loss: 0.9378 - val_loss: 0.8457\n",
            "Epoch 15/20\n",
            "128/128 - 13s - loss: 0.9371 - val_loss: 0.8484\n",
            "Epoch 16/20\n",
            "128/128 - 13s - loss: 0.9365 - val_loss: 0.8508\n",
            "Epoch 17/20\n",
            "128/128 - 13s - loss: 0.9359 - val_loss: 0.8528\n",
            "Epoch 18/20\n",
            "128/128 - 13s - loss: 0.9354 - val_loss: 0.8540\n",
            "Epoch 19/20\n",
            "128/128 - 13s - loss: 0.9348 - val_loss: 0.8545\n",
            "Epoch 20/20\n",
            "128/128 - 13s - loss: 0.9340 - val_loss: 0.8540\n",
            "1906/1906 [==============================] - 3s 2ms/sample - loss: 0.8540\n",
            "\n",
            "Layer: 4\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "pqc_24 (PQC)                 (None, 1)                 60        \n",
            "=================================================================\n",
            "Total params: 60\n",
            "Trainable params: 60\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 128 samples, validate on 1906 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75d8ce4eb8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "WARNING: AutoGraph could not transform <bound method ParameterShift.differentiate_analytic of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f75d8ce4eb8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unexpected indent (<unknown>, line 74)\n",
            "128/128 - 20s - loss: 1.0544 - val_loss: 0.8326\n",
            "Epoch 2/20\n",
            "128/128 - 19s - loss: 1.0302 - val_loss: 0.8224\n",
            "Epoch 3/20\n",
            "128/128 - 19s - loss: 1.0079 - val_loss: 0.8144\n",
            "Epoch 4/20\n",
            "128/128 - 19s - loss: 0.9876 - val_loss: 0.8087\n",
            "Epoch 5/20\n",
            "128/128 - 19s - loss: 0.9695 - val_loss: 0.8054\n",
            "Epoch 6/20\n",
            "128/128 - 19s - loss: 0.9537 - val_loss: 0.8045\n",
            "Epoch 7/20\n",
            "128/128 - 19s - loss: 0.9404 - val_loss: 0.8061\n",
            "Epoch 8/20\n",
            "128/128 - 19s - loss: 0.9296 - val_loss: 0.8100\n",
            "Epoch 9/20\n",
            "128/128 - 19s - loss: 0.9214 - val_loss: 0.8158\n",
            "Epoch 10/20\n",
            "128/128 - 19s - loss: 0.9157 - val_loss: 0.8233\n",
            "Epoch 11/20\n",
            "128/128 - 19s - loss: 0.9123 - val_loss: 0.8317\n",
            "Epoch 12/20\n",
            "128/128 - 19s - loss: 0.9109 - val_loss: 0.8401\n",
            "Epoch 13/20\n",
            "128/128 - 19s - loss: 0.9108 - val_loss: 0.8476\n",
            "Epoch 14/20\n",
            "128/128 - 19s - loss: 0.9114 - val_loss: 0.8534\n",
            "Epoch 15/20\n",
            "128/128 - 19s - loss: 0.9121 - val_loss: 0.8570\n",
            "Epoch 16/20\n",
            "128/128 - 19s - loss: 0.9123 - val_loss: 0.8584\n",
            "Epoch 17/20\n",
            "128/128 - 19s - loss: 0.9119 - val_loss: 0.8577\n",
            "Epoch 18/20\n",
            "128/128 - 19s - loss: 0.9108 - val_loss: 0.8553\n",
            "Epoch 19/20\n",
            "128/128 - 19s - loss: 0.9092 - val_loss: 0.8517\n",
            "Epoch 20/20\n",
            "128/128 - 19s - loss: 0.9072 - val_loss: 0.8473\n",
            "1906/1906 [==============================] - 4s 2ms/sample - loss: 0.8473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTDPaUmNXhhQ",
        "colab_type": "code",
        "outputId": "a13268ae-cc2b-4e67-bdef-1283a9e8476f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(training_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f75d8ac0828>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX2Ln6dfUb47",
        "colab_type": "text"
      },
      "source": [
        "As already pointed out in the MNIST example notebook, a classical neural network is hard to beat on a simple learning task like this, especially with a basic data encoding scheme as used above. In general, layerwise learning can be used in arbitrary configurations that allow successively stacking and training layers, and it is independent of the data encoding scheme used - so feel free to play with more elaborate data sets as well!"
      ]
    }
  ]
}